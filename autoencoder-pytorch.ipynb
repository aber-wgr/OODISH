{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing an Autoencoder in PyTorch\n",
    "===\n",
    "\n",
    "This is adapted from the workbook provided alongside the article \"Implementing an Autoencoder in Pytorch\" which can be found [here](https://medium.com/pytorch/implementing-an-autoencoder-in-pytorch-19baa22647d1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our seed and other configurations for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    platform = \"cuda\"\n",
    "else:\n",
    "    plaform = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the batch size, the number of training epochs, and the learning rate. Batch size has to be reasonably low as we can't fit a huge number of these images into VRAM on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 416\n",
    "height = 512\n",
    "\n",
    "image_size = width * height\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "ImageFolder is used to load the basic distribution images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "root_dir = \"../../Data/OPTIMAM_NEW/png_images\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "An autoencoder is a type of neural network that finds the function mapping the features x to itself. This objective is known as reconstruction, and an autoencoder accomplishes this through the following process: (1) an encoder learns the data representation in lower-dimension space, i.e. extracting the most salient features of the data, and (2) a decoder learns to reconstruct the original data based on the learned representation by the encoder.\n",
    "\n",
    "We define our autoencoder class with fully connected layers for both its encoder and decoder components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 512\n",
    "code_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=hidden_layer_size\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=hidden_layer_size, out_features=code_size\n",
    "        )\n",
    "        self.decoder_hidden_layer = nn.Linear(\n",
    "            in_features=code_size, out_features=hidden_layer_size\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=hidden_layer_size, out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.sigmoid(code)\n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.sigmoid(activation)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using our defined autoencoder class, we have the following things to do:\n",
    "    1. We configure which device we want to run on.\n",
    "    2. We instantiate an `AE` object.\n",
    "    3. We define our optimizer.\n",
    "    4. We define our reconstruction loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(platform)\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_shape=image_size).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our autoencoder for our specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/50, recon loss = 0.05263413\n",
      "epoch : 2/50, recon loss = 0.03814048\n",
      "epoch : 3/50, recon loss = 0.03269933\n",
      "epoch : 4/50, recon loss = 0.02762102\n",
      "epoch : 5/50, recon loss = 0.02483114\n",
      "epoch : 6/50, recon loss = 0.02628068\n",
      "epoch : 7/50, recon loss = 0.02528697\n",
      "epoch : 8/50, recon loss = 0.02435478\n",
      "epoch : 9/50, recon loss = 0.02464137\n",
      "epoch : 10/50, recon loss = 0.02452361\n",
      "epoch : 11/50, recon loss = 0.02399984\n",
      "epoch : 12/50, recon loss = 0.02446257\n",
      "epoch : 13/50, recon loss = 0.02436928\n",
      "epoch : 14/50, recon loss = 0.02438553\n",
      "epoch : 15/50, recon loss = 0.02393048\n",
      "epoch : 16/50, recon loss = 0.02404758\n",
      "epoch : 17/50, recon loss = 0.02401521\n",
      "epoch : 18/50, recon loss = 0.02425481\n",
      "epoch : 19/50, recon loss = 0.02419089\n",
      "epoch : 20/50, recon loss = 0.02433458\n",
      "epoch : 21/50, recon loss = 0.02393543\n",
      "epoch : 22/50, recon loss = 0.02396328\n",
      "epoch : 23/50, recon loss = 0.02407317\n",
      "epoch : 24/50, recon loss = 0.02429132\n",
      "epoch : 25/50, recon loss = 0.02380170\n",
      "epoch : 26/50, recon loss = 0.02380443\n",
      "epoch : 27/50, recon loss = 0.02446838\n",
      "epoch : 28/50, recon loss = 0.02420490\n",
      "epoch : 29/50, recon loss = 0.02417899\n",
      "epoch : 30/50, recon loss = 0.02381438\n",
      "epoch : 31/50, recon loss = 0.02381264\n",
      "epoch : 32/50, recon loss = 0.02403047\n",
      "epoch : 33/50, recon loss = 0.02355428\n",
      "epoch : 34/50, recon loss = 0.02407222\n",
      "epoch : 35/50, recon loss = 0.02404132\n",
      "epoch : 36/50, recon loss = 0.02436884\n",
      "epoch : 37/50, recon loss = 0.02372017\n",
      "epoch : 38/50, recon loss = 0.02419384\n",
      "epoch : 39/50, recon loss = 0.02396021\n",
      "epoch : 40/50, recon loss = 0.02384675\n",
      "epoch : 41/50, recon loss = 0.02390791\n",
      "epoch : 42/50, recon loss = 0.02438359\n",
      "epoch : 43/50, recon loss = 0.02394310\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.view(-1, image_size).to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    \n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract some test examples to reconstruct using our trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "root_dir = \"../../Data/OPTIMAM_NEW/png_images\"\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=5, shuffle=True\n",
    ")\n",
    "\n",
    "test_examples = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features in test_loader:\n",
    "        batch_features = batch_features[0]\n",
    "        test_examples = batch_features.view(-1, image_size).to(device)\n",
    "        reconstruction = model(test_examples)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's try to reconstruct some test images using our trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    number = 5\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for index in range(number):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, number, index + 1)\n",
    "        copyback = test_examples[index].cpu()\n",
    "        plt.imshow(copyback.numpy().reshape(height, width))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, number, index + 1 + number)\n",
    "        recon_copyback = reconstruction[index].cpu()\n",
    "        plt.imshow(recon_copyback.numpy().reshape(height, width))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
                                                                                                                                                                                                                                                          