{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder Anomaly Testing\n",
    "===\n",
    "\n",
    "This is rebuilt from the \"Collecting Network Statistics\" notebook. The goal of this notebook is to collect together a set of in-distribution and out-of-distribution images and confirm that the model can distinguish them with a high degree of accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from model import SplitAutoencoder,ExtensibleEncoder,ExtensibleDecoder\n",
    "from CustomDataSet import CustomDataSet,CustomDataSetWithError\n",
    "import os\n",
    "\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our seed and other configurations for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = 42\n",
    "seed = 2662\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    platform = \"cuda\"\n",
    "else:\n",
    "    platform = \"cpu\"\n",
    "#platform = \"cpu\"\n",
    "print(platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the batch size, the number of training epochs, and the learning rate. Batch size has to be reasonably low as we can't fit a huge number of these images into VRAM on my laptop.\n",
    "\n",
    "Image size can be set here as I'm automatically resizing the images in my extraction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "\n",
    "image_size = width * height\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "#meta-parameters\n",
    "l2_decay = 0.0\n",
    "dropout_rate = 0.0\n",
    "code_sides = [10]\n",
    "convolution_filters = 32\n",
    "\n",
    "model_path = \"../../Data/OPTIMAM_NEW/model\" + str(width) + \"_\" + str(code_sides[0]) + \"_\" + str(convolution_filters) + \"_\" + str(dropout_rate) + \"_\" + str(l2_decay) + \".pt\"\n",
    "\n",
    "#image_count = 500\n",
    "image_count = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Base Distribution Information\n",
    "\n",
    "First we run the model on the entire original distribution and gather statistics on the loss values, encodings etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor,Grayscale\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.0,65535.0)\n",
    "    ])\n",
    "\n",
    "#root_dir = \"../../Data/OPTIMAM_NEW/png_images/casewise/ScreeningMammography/\"+str(width)\n",
    "root_dir = \"../../Data/OPTIMAM_NEW/png_images/lesions/\"\n",
    "train_dataset = CustomDataSet(root_dir, transform)\n",
    "if (image_count == -1):\n",
    "    train_dataset_subset = train_dataset\n",
    "    image_count = len(train_dataset)\n",
    "else:\n",
    "    train_dataset_subset = torch.utils.data.Subset(train_dataset, numpy.random.choice(len(train_dataset), image_count, replace=False))\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset_subset, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(platform)\n",
    "\n",
    "code_size = code_sides[0] * code_sides[0]\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "losses = [None] * len(train_dataset_subset)\n",
    "encodings = [None] * len(train_dataset_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the saved model\n",
    "model = torch.load(model_path,map_location=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our autoencoder on the entire dataset and store the encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for batch_features in train_loader:\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "\n",
    "        # compute reconstructions\n",
    "        code = model.encoder(batch_features)\n",
    "        outputs = model.decoder(code)\n",
    "        \n",
    "        code_reshaped = code.detach().cpu().numpy()[0]\n",
    "        code_reshaped.reshape(code_size)\n",
    "\n",
    "        encodings[count] = code_reshaped\n",
    "\n",
    "        # compute training reconstruction loss\n",
    "        error_criterion = criterion(outputs,batch_features)\n",
    "\n",
    "        losses[count] = error_criterion.cpu().numpy()\n",
    "\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate the encoding statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encodings))\n",
    "print(len(encodings[0]))\n",
    "print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = np.mean(encodings,axis=0)\n",
    "feature_stds = np.std(encodings,axis=0)\n",
    "print(len(feature_means))\n",
    "print(len(feature_stds))\n",
    "\n",
    "mse_min = np.amin(losses)\n",
    "mse_max = np.amax(losses)\n",
    "mse_mean = np.mean(losses)\n",
    "print(\"MSE Min/Mean/Max:\" + str(mse_min) + \"/\" + str(mse_mean) + \"/\" + str(mse_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the compiled statistics to an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    np_losses = np.asarray(losses)\n",
    "\n",
    "    np_compiled = np.concatenate((np_losses[:, np.newaxis], encodings), axis=1)\n",
    "\n",
    "    suffix =  \"_\" + str(width) + \"_\" + str(code_sides[0]) + \"_\" + str(convolution_filters) + \"_\" + str(dropout_rate) + \"_\" + str(l2_decay)\n",
    "    \n",
    "    np.savetxt('encodings' + suffix + '.csv', encodings, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('losses' + suffix + '.csv', np_losses, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('combined' + suffix + '.csv', np_compiled, delimiter=',',fmt='%10.5f',newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarials\n",
    "\n",
    "We have 2 Datasets (mammographic and non-mammographic) and 3 DataLoaders - Clean Mammo, Distorted Mammo, and Non-Mammo. The goal here is to build an analogously large set of OOD images and test to what degree the autoencoder is capable of detecting the distortions.\n",
    "\n",
    "The first method for doing this builds a large set of all the datasets classified into In-Distribution and Out-Of-Distribution and determine the accuracy rating of the model as a classifier. The second generates a set of distorted mammographic images at specified distances from the distribution, along with a value roughly analogous to that distortion level. This second method is intended to determine the range in distribution space at which the model becomes able to distinguish, as well as the degree of \"grey area\" between in and out of distribution (as detected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    trigger_chance = 0.8\n",
    "\n",
    "    PIL_transforms = torchvision.transforms.RandomApply([\n",
    "        torchvision.transforms.RandomAffine(degrees=15,translate=(0.4,0.4),shear=60),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.GaussianBlur(kernel_size=5),\n",
    "        torchvision.transforms.ColorJitter(brightness=0.8,contrast=0.8)\n",
    "        ],p=trigger_chance)\n",
    "\n",
    "    adversarial_transform = torchvision.transforms.Compose([\n",
    "        PIL_transforms,\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(0.0,65535.0),\n",
    "         torchvision.transforms.RandomErasing(p=trigger_chance),\n",
    "        ])\n",
    "\n",
    "    adversarial_image_count = image_count\n",
    "    adversarial_dataset = CustomDataSetWithError(root_dir, adversarial_transform)\n",
    "    \n",
    "    #adversarial_subset = torch.utils.data.Subset(adversarial_dataset, np.random.choice(len(adversarial_dataset), adversarial_image_count, replace=False))\n",
    "\n",
    "    adversarial_loader = torch.utils.data.DataLoader(\n",
    "        adversarial_dataset, shuffle=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the first (mixed) set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    adversarial_iterator = iter(adversarial_loader)\n",
    "    genuine_iterator = iter(train_loader)\n",
    "    mixed_set_scale = 10\n",
    "    \n",
    "    mixed_set = []\n",
    "    mixed_set_np = []\n",
    "    mixed_class_set = []\n",
    "    mixed_error_set = []\n",
    "    for i in range(mixed_set_scale):\n",
    "        r = torch.rand(1)\n",
    "        if(r.item() > 0.5):\n",
    "            adversarial_t = next(adversarial_iterator)\n",
    "            adversarial = adversarial_t[0].cpu()\n",
    "            adversarial_np = adversarial.numpy().reshape(width,height)\n",
    "            adv_error = adversarial_t[1]\n",
    "            mixed_class_set.append(0)\n",
    "            mixed_set_np.append(adversarial_np)\n",
    "            mixed_set.append(adversarial)\n",
    "            mixed_error_set.append(adv_error)\n",
    "        else:\n",
    "            genuine = next(genuine_iterator).cpu()\n",
    "            genuine_np = genuine.numpy().reshape(width,height)\n",
    "            mixed_class_set.append(1)\n",
    "            mixed_set.append(genuine)\n",
    "            mixed_set_np.append(genuine_np)\n",
    "            mixed_error_set.append(0.0) # genuine, so no drift\n",
    "        \n",
    "    mixed_code_set = []\n",
    "    mixed_reconstruction_set = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model on the mixed set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for mixed_item in mixed_set:\n",
    "        mixed_example = mixed_item.to(device)\n",
    "        \n",
    "        n_code = model.encoder(mixed_example)\n",
    "        reconstruction = model(mixed_example)\n",
    "        \n",
    "        mixed_code_set.append(n_code.cpu())\n",
    "        mixed_reconstruction_set.append(reconstruction.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, measure the loss and feature statistics for the adversarials:            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_errors = []\n",
    "for image_n in range(mixed_set_scale):\n",
    "    print(np.shape(mixed_reconstruction_set[image_n]))\n",
    "    recon = mixed_reconstruction_set[image_n][0][0]\n",
    "    original = mixed_set[image_n][0][0]\n",
    "    t_se = 0.0\n",
    "    q = []\n",
    "    n = 0\n",
    "    for y in range(height):\n",
    "        p = []\n",
    "        for x in range(width):\n",
    "            base_pixel = original[y][x].item()\n",
    "            recon_pixel = recon[y][x].item()\n",
    "            #specifically ignore all-black pixels\n",
    "            p.append(se)\n",
    "            if (base_pixel != 0.0):\n",
    "                se = (recon_pixel - base_pixel) ** 2\n",
    "                t_se = t_se + se\n",
    "                n = n + 1\n",
    "        q.append(p)\n",
    "    #print(q)\n",
    "    \n",
    "    mse = t_se / n\n",
    "    \n",
    "    print(\"n:\" + str(n))\n",
    "    print(\"TSE:\" + str(t_se))\n",
    "    print(\"MSE:\" + str(mse))\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.ma.masked_equal(feature_stds,0)\n",
    "print(feature_means)\n",
    "for i in range(mixed_set_scale):\n",
    "    adv_divergence = (mixed_code_set[i] - feature_means) / g\n",
    "    print(np.max(adv_divergence,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    number = mixed_set_scale\n",
    "    #number = 5\n",
    "    plt.figure(figsize=(25, 9))\n",
    "    for index in range(number):\n",
    "        # display original\n",
    "        ax = plt.subplot(3, number, index + 1)\n",
    "        test_examples = mixed_set\n",
    "        copyback = test_examples[index].cpu()\n",
    "        plt.imshow(copyback.reshape(height, width))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display codes\n",
    "        ax = plt.subplot(3, number, index + number + 1)\n",
    "        codes = mixed_code_set\n",
    "        code_copyback = codes[index].cpu()\n",
    "        plt.imshow(code_copyback.reshape(code_sides[0],code_sides[0]))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(3, number, index + (number*2) + 1)\n",
    "        reconstruction = mixed_reconstruction_set\n",
    "        recon_copyback = reconstruction[index].cpu()\n",
    "        plt.imshow(recon_copyback.reshape(height, width))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    out_path = \"adv_output\"+ str(width) + \"_\"  + str(code_sides[0]) + \"_\" + str(convolution_filters) + \"_\" + str(dropout_rate) + \"_\" + str(l2_decay) +\".png\"\n",
    "    plt.savefig(out_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
