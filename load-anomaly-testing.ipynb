{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder Anomaly Testing\n",
    "===\n",
    "\n",
    "This is rebuilt from the \"Collecting Network Statistics\" notebook. The goal of this notebook is to collect together a set of in-distribution and out-of-distribution images and confirm that the model can distinguish them with a high degree of accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from pytorch_msssim import ssim,ms_ssim,SSIM\n",
    "from model import SplitAutoencoder,ExtensibleEncoder,ExtensibleDecoder\n",
    "from CustomDataSet import CustomDataSet,CustomDataSetWithError\n",
    "import os\n",
    "\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our seed and other configurations for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "#seed = 2662\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "basis = \"holdout\"\n",
    "distribution = \"prostate\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    platform = \"cuda\"\n",
    "else:\n",
    "    platform = \"cpu\"\n",
    "#platform = \"cpu\"\n",
    "print(platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the batch size, the number of training epochs, and the learning rate. Batch size has to be reasonably low as we can't fit a huge number of these images into VRAM on my laptop.\n",
    "\n",
    "Image size can be set here as I'm automatically resizing the images in my extraction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "\n",
    "image_size = width * height\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "#meta-parameters\n",
    "l2_decay = 0.0\n",
    "dropout_rate = 0.0\n",
    "code_sides = [20]\n",
    "convolution_filters = 32\n",
    "\n",
    "model_extension = str(width) + \"_\" + str(code_sides[0]) + \"_\" + str(convolution_filters) + \"_\" + str(dropout_rate) + \"_\" + str(l2_decay)\n",
    "full_extension = \"_\" + basis + \"_\" + distribution + \"_\" + model_extension\n",
    "\n",
    "model_path = \"../../Data/OPTIMAM_NEW/model\" + full_extension + \".pt\"\n",
    "\n",
    "#image_count = 500\n",
    "image_count = -1\n",
    "\n",
    "validation_split = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Base Distribution Information\n",
    "\n",
    "First we run the model on the entire original distribution and gather statistics on the loss values, encodings etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor,Normalize\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.0,65535.0)\n",
    "    ])\n",
    "\n",
    "if distribution==\"screening\":\n",
    "    root_dir = \"../../Data/OPTIMAM_NEW/png_images/casewise/ScreeningMammography/\" + str(width)\n",
    "else if distribution==\"\":\n",
    "    root_dir = \"../../Data/OPTIMAM_NEW/png_images/lesions\"\n",
    "else:\n",
    "    root_dir = \"../../Data/Prostate/prostate-cancer-grade-assessment/train_images\"\n",
    "train_dataset = CustomDataSet(root_dir, transform)\n",
    "\n",
    "if (image_count == -1):\n",
    "    train_dataset_subset = train_dataset\n",
    "    image_count = len(train_dataset)\n",
    "else:\n",
    "    train_dataset_subset = torch.utils.data.Subset(train_dataset, numpy.random.choice(len(train_dataset), image_count, replace=False))\n",
    "\n",
    "train_subset_idx = np.random.choice(len(train_dataset), int(image_count * validation_split), replace=False)\n",
    "\n",
    "n = np.arange(len(train_dataset))\n",
    "mask = np.ones(len(train_dataset), dtype=bool)\n",
    "mask[train_subset_idx] = False\n",
    "holdout_subset_idx = n[mask]\n",
    "\n",
    "dataset_size = len(train_dataset_subset)\n",
    "      \n",
    "t_subset = torch.utils.data.Subset(train_dataset_subset, train_subset_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    t_subset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "h_subset = torch.utils.data.Subset(train_dataset_subset, holdout_subset_idx)\n",
    "\n",
    "holdout_loader = torch.utils.data.DataLoader(\n",
    "    h_subset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_subset_idx)\n",
    "print(holdout_subset_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(platform)\n",
    "\n",
    "code_size = code_sides[0] * code_sides[0]\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "features = [None] * len(t_subset)\n",
    "losses = [None] * len(t_subset)\n",
    "encodings = [None] * len(t_subset)\n",
    "outputs = [None] * len(t_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the saved model\n",
    "model = torch.load(model_path,map_location=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our autoencoder on the entire dataset and store the encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for batch_features in train_loader:\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "\n",
    "        features[count] = batch_features.cpu()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        code = model.encoder(batch_features)\n",
    "        output = model.decoder(code)\n",
    "        \n",
    "        outputs[count] = output.cpu()\n",
    "        \n",
    "        code_reshaped = code.detach().cpu().numpy()[0]\n",
    "        code_reshaped.reshape(code_size)\n",
    "\n",
    "        encodings[count] = code_reshaped\n",
    "\n",
    "        # compute training reconstruction loss\n",
    "        error_criterion = criterion(output,batch_features)\n",
    "\n",
    "        losses[count] = error_criterion.cpu().numpy()\n",
    "\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate the encoding statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encodings))\n",
    "print(len(encodings[0]))\n",
    "print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_min = np.amin(losses)\n",
    "mse_max = np.amax(losses)\n",
    "mse_mean = np.mean(losses)\n",
    "mse_std = np.std(losses)\n",
    "print(\"MSE Min/Mean/Max/SD:\" + str(mse_min) + \"/\" + str(mse_mean) + \"/\" + str(mse_max) + \"/\" + str(mse_std)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_n = np.stack(features)\n",
    "outputs_n = np.stack(outputs)\n",
    "print(features_n.shape)\n",
    "print(outputs_n.shape)\n",
    "\n",
    "pre_ssims = [None] * len(encodings)\n",
    "\n",
    "ssim_module = SSIM(data_range=1.0, size_average=False, channel=3)\n",
    "\n",
    "for i in range(len(encodings)):\n",
    "    features_s = features_n[i].reshape(1,1,height,width).repeat(3,1)\n",
    "    outputs_s = outputs_n[i].reshape(1,1,height,width).repeat(3,1)\n",
    "    ssim_f = ssim_module(torch.from_numpy(features_s), torch.from_numpy(outputs_s))\n",
    "    pre_ssims[i] = ssim_f.item()\n",
    "    \n",
    "ssim_min = np.amin(pre_ssims)\n",
    "ssim_max = np.amax(pre_ssims)\n",
    "ssim_mean = np.mean(pre_ssims)\n",
    "ssim_sd = np.std(pre_ssims)\n",
    "print(\"SSIM Min/Mean/Max/SD:\" + str(ssim_min) + \"/\" + str(ssim_mean) + \"/\" + str(ssim_max) + \"/\" + str(ssim_sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the compiled statistics to an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    np_losses = np.asarray(losses)\n",
    "    np_pre_ssims = np.asarray(pre_ssims)\n",
    "    np_compiled = np.concatenate((np_losses[:, np.newaxis], encodings), axis=1)\n",
    "\n",
    "    suffix = full_extension\n",
    "    \n",
    "    np.savetxt('base_encodings' + suffix + '.csv', encodings, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('base_losses' + suffix + '.csv', np_losses, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('base_ssim' + suffix + '.csv', np_pre_ssims, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('base_combined' + suffix + '.csv', np_compiled, delimiter=',',fmt='%10.5f',newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarials\n",
    "\n",
    "We have 2 Datasets (mammographic and non-mammographic) and 3 DataLoaders - Clean Mammo, Distorted Mammo, and Non-Mammo. The goal here is to build an analogously large set of OOD images and test to what degree the autoencoder is capable of detecting the distortions.\n",
    "\n",
    "The first method for doing this builds a large set of all the datasets classified into In-Distribution and Out-Of-Distribution and determine the accuracy rating of the model as a classifier. The second generates a set of distorted mammographic images at specified distances from the distribution, along with a value roughly analogous to that distortion level. This second method is intended to determine the range in distribution space at which the model becomes able to distinguish, as well as the degree of \"grey area\" between in and out of distribution (as detected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    trigger_chance = 0.4\n",
    "\n",
    "    PIL_transforms = torchvision.transforms.RandomApply([\n",
    "        torchvision.transforms.RandomAffine(degrees=10,translate=(0.2,0.2),shear=25),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.GaussianBlur(kernel_size=3),\n",
    "        torchvision.transforms.ColorJitter(brightness=0.1,contrast=0.1)\n",
    "        ],p=trigger_chance)\n",
    "    \n",
    "    tensor_transforms = torchvision.transforms.RandomApply([\n",
    "        torchvision.transforms.RandomErasing(p=0.8,value='random'),\n",
    "        torchvision.transforms.Lambda(lambda x : x + torch.randn_like(2 * x))\n",
    "        ],p=trigger_chance)\n",
    "\n",
    "    adversarial_transform = torchvision.transforms.Compose([\n",
    "        PIL_transforms,\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(0.0,1.0),\n",
    "        tensor_transforms\n",
    "        ])\n",
    "   \n",
    "    adversarial_image_count = image_count\n",
    "    adversarial_dataset = CustomDataSetWithError(root_dir, adversarial_transform)\n",
    "    \n",
    "    if(basis == \"holdout\"): \n",
    "        a_subset = torch.utils.data.Subset(adversarial_dataset, holdout_subset_idx)\n",
    "    else:\n",
    "        a_subset = torch.utils.data.Subset(adversarial_dataset, train_subset_idx)\n",
    "\n",
    "    adversarial_loader = torch.utils.data.DataLoader(\n",
    "        a_subset,shuffle=True\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the first (mixed) set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    adversarial_iterator = iter(adversarial_loader)\n",
    "    genuine_iterator = iter(holdout_loader)\n",
    "    mixed_set_scale = 300\n",
    "    \n",
    "    mixed_set = []\n",
    "    mixed_set_np = []\n",
    "    mixed_class_set = []\n",
    "    mixed_error_set = []\n",
    "    for i in range(mixed_set_scale):\n",
    "        r = torch.rand(1)\n",
    "        if(r.item() > 0.5):\n",
    "            adversarial_t = next(adversarial_iterator)\n",
    "            adversarial = adversarial_t[0].cpu()\n",
    "            adversarial_np = adversarial.numpy().reshape(width,height)\n",
    "            adv_error = adversarial_t[1]\n",
    "            mixed_class_set.append(0)\n",
    "            mixed_set_np.append(adversarial_np)\n",
    "            mixed_set.append(adversarial)\n",
    "            mixed_error_set.append(adv_error.item())\n",
    "        else:\n",
    "            genuine = next(genuine_iterator).cpu()\n",
    "            genuine_np = genuine.numpy().reshape(width,height)\n",
    "            mixed_class_set.append(1)\n",
    "            mixed_set.append(genuine)\n",
    "            mixed_set_np.append(genuine_np)\n",
    "            mixed_error_set.append(0.0) # genuine, so no drift\n",
    "        \n",
    "    mixed_code_set = []\n",
    "    mixed_reconstruction_set = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model on the mixed set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for mixed_item in mixed_set:\n",
    "        mixed_example = mixed_item.to(device)\n",
    "        \n",
    "        n_code = model.encoder(mixed_example)\n",
    "        reconstruction = model(mixed_example)\n",
    "        \n",
    "        mixed_code_set.append(n_code.cpu())\n",
    "        mixed_reconstruction_set.append(reconstruction.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, measure the loss and feature statistics for the adversarials:            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First calculate the MSE for all the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_errors = []\n",
    "\n",
    "for image_n in range(mixed_set_scale):\n",
    "    recon = mixed_reconstruction_set[image_n][0][0]\n",
    "    original = mixed_set[image_n][0][0]\n",
    "    t_se = 0.0\n",
    "    q = []\n",
    "    n = 0\n",
    "    t_min = 65535.0\n",
    "    t_max = 0.0\n",
    "    for y in range(height):\n",
    "        p = []\n",
    "        for x in range(width):\n",
    "            base_pixel = original[y][x].item()\n",
    "            recon_pixel = recon[y][x].item()\n",
    "            #specifically ignore all-black pixels\n",
    "            if(base_pixel < t_min):\n",
    "                t_min = base_pixel\n",
    "            if(base_pixel > t_max):\n",
    "                t_max = base_pixel\n",
    "            se = (recon_pixel - base_pixel) ** 2\n",
    "            p.append(se)\n",
    "            t_se = t_se + se\n",
    "            n = n + 1\n",
    "        q.append(p)\n",
    "    #print(q)\n",
    "    \n",
    "    mse = t_se / n\n",
    "    \n",
    "    mean_squared_errors.append(mse)\n",
    "    \n",
    "post_mse_min = np.amin(mean_squared_errors)\n",
    "post_mse_max = np.amax(mean_squared_errors)\n",
    "post_mse_mean = np.mean(mean_squared_errors)\n",
    "post_mse_std = np.std(mean_squared_errors)\n",
    "print(\"Prediction MSE Min/Mean/Max/SD:\" + str(post_mse_min) + \"/\" + str(post_mse_mean) + \"/\" + str(post_mse_max) + \"/\" + str(post_mse_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And attempt to predict classes based on MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_threshold = mse_mean + (2 * mse_std)\n",
    "\n",
    "predicted_class = []\n",
    "for i in range(mixed_set_scale):\n",
    "    if(mean_squared_errors[i]<mse_threshold):\n",
    "        predicted_class.append(1) # distribution\n",
    "    else:\n",
    "        predicted_class.append(0) # adversarial\n",
    "\n",
    "mixed_class_np = np.asarray(mixed_class_set)\n",
    "predicted_class_np = np.asarray(predicted_class)\n",
    "print(mixed_class_np)\n",
    "print(predicted_class_np)\n",
    "matches = (mixed_class_np == predicted_class_np)\n",
    "print(matches)\n",
    "hits = np.count_nonzero(matches)\n",
    "misses = mixed_set_scale - hits\n",
    "accuracy = hits / mixed_set_scale\n",
    "print(\"Accuracy:\" + str(accuracy))\n",
    "\n",
    "fails = np.where(matches==False)\n",
    "print(fails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STRUCTURAL SIMILARITY INDEX (SSIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate RGB versions of the base and recreated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_set_s = np.stack(mixed_set).reshape(mixed_set_scale,1,height,width).repeat(3,1)\n",
    "mixed_recon_s = np.stack(mixed_reconstruction_set).reshape(mixed_set_scale,1,height,width).repeat(3,1)\n",
    "print(mixed_set_s.shape)\n",
    "print(mixed_recon_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use that for Structural Similarity Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_t = ssim(torch.from_numpy(mixed_set_s), torch.from_numpy(mixed_recon_s), data_range=1.0, size_average = False)\n",
    "ms_ssim_t = ms_ssim(torch.from_numpy(mixed_set_s), torch.from_numpy(mixed_recon_s), data_range=1.0, size_average = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_ssim_min = np.amin(ssim_t.numpy())\n",
    "post_ssim_max = np.amax(ssim_t.numpy())\n",
    "post_ssim_mean = np.mean(ssim_t.numpy())\n",
    "post_ssim_std = np.std(ssim_t.numpy())\n",
    "print(\"Prediction SSIM Min/Mean/Max/SD:\" + str(post_ssim_min) + \"/\" + str(post_ssim_mean) + \"/\" + str(post_ssim_max) + \"/\" + str(post_ssim_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    np_post_losses = np.asarray(mean_squared_errors)\n",
    "    np_post_ssims = np.asarray(ssim_t)\n",
    "    np_distances = np.asarray(mixed_error_set)\n",
    "    \n",
    "    suffix = full_extension\n",
    "    \n",
    "    np.savetxt('mixed_losses' + suffix + '.csv', np_post_losses, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('mixed_ssim' + suffix + '.csv', np_post_ssims, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('mixed_distance' + suffix + '.csv', np_distances, delimiter=',',fmt='%10.5f',newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to predict the class based on the SSIM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_threshold = ssim_mean - (2 * ssim_sd)\n",
    "#ssim_threshold = ssim_min\n",
    "predicted_class = []\n",
    "for i in range(mixed_set_scale):\n",
    "    if(ssim_t[i]>ssim_threshold):\n",
    "        predicted_class.append(1) # distribution\n",
    "    else:\n",
    "        predicted_class.append(0) # adversarial\n",
    "\n",
    "mixed_class_np = np.asarray(mixed_class_set)\n",
    "predicted_class_np = np.asarray(predicted_class)\n",
    "print(mixed_class_np)\n",
    "print(predicted_class_np)\n",
    "matches = (mixed_class_np == predicted_class_np)\n",
    "print(matches)\n",
    "hits = np.count_nonzero(matches)\n",
    "misses = mixed_set_scale - hits\n",
    "accuracy = hits / mixed_set_scale\n",
    "print(\"Accuracy:\" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the first 10 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mixed_set_g = np.stack(mixed_set).reshape(mixed_set_scale,height,width,1).repeat(3,3)\n",
    "    mixed_recon_g = np.stack(mixed_reconstruction_set).reshape(mixed_set_scale,height,width,1).repeat(3,3)\n",
    "    print(mixed_set_g.shape)\n",
    "    print(mixed_recon_g.shape)\n",
    "\n",
    "    #number = mixed_set_scale\n",
    "    number = 10\n",
    "    plt.figure(figsize=(25, 9))\n",
    "    for index in range(number):\n",
    "        # display original\n",
    "        ax = plt.subplot(3, number, index + 1)\n",
    "        test_examples = mixed_set_g\n",
    "        copyback = test_examples[index]\n",
    "        print(copyback.shape)\n",
    "        plt.imshow(copyback)\n",
    "        #plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display codes\n",
    "        ax = plt.subplot(3, number, index + number + 1)\n",
    "        codes = mixed_code_set\n",
    "        code_copyback = codes[index].cpu()\n",
    "        plt.imshow(code_copyback.reshape(code_sides[0],code_sides[0]))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(3, number, index + (number*2) + 1)\n",
    "        reconstruction = mixed_recon_g\n",
    "        recon_copyback = reconstruction[index]\n",
    "        plt.imshow(recon_copyback)\n",
    "        #plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    out_path = \"adv_output\" + full_extension + \".png\"\n",
    "    plt.savefig(out_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the failures from the MSE calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    #number = mixed_set_scale\n",
    "    number = len(fails[0])\n",
    "    plt.figure(figsize=(25, 9))\n",
    "    for index in range(number):\n",
    "        image_index = fails[0][index]\n",
    "        # display original\n",
    "        ax = plt.subplot(3, number, index + 1)\n",
    "        test_examples = mixed_set_g\n",
    "        copyback = test_examples[image_index]\n",
    "        print(copyback.shape)\n",
    "        plt.imshow(copyback)\n",
    "        #plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display codes\n",
    "        ax = plt.subplot(3, number, index + number + 1)\n",
    "        codes = mixed_code_set\n",
    "        code_copyback = codes[image_index].cpu()\n",
    "        plt.imshow(code_copyback.reshape(code_sides[0],code_sides[0]))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(3, number, index + (number*2) + 1)\n",
    "        reconstruction = mixed_recon_g\n",
    "        recon_copyback = reconstruction[image_index]\n",
    "        plt.imshow(recon_copyback)\n",
    "        #plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    out_path = \"fail_output\" + full_extension + \".png\"\n",
    "    plt.savefig(out_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot projected error against reconstructed MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(mixed_error_set, np_post_losses, 'o', color='black');\n",
    "out_path = \"graph_output\" + full_extension + \".png\"\n",
    "plt.savefig(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
