{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting Network Statistics in Pytorch\n",
    "===\n",
    "\n",
    "This is adapted from the workbook provided alongside the article \"Implementing an Autoencoder in Pytorch\" which can be found [here](https://medium.com/pytorch/implementing-an-autoencoder-in-pytorch-19baa22647d1). The purpose is to load generated trained models from the Autoencoder implementation and collect encoding statistics for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "from model import SplitAutoencoder,ExtensibleEncoder,ExtensibleDecoder\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our seed and other configurations for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    platform = \"cuda\"\n",
    "else:\n",
    "    platform = \"cpu\"\n",
    "#platform = \"cpu\"\n",
    "print(platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the batch size, the number of training epochs, and the learning rate. Batch size has to be reasonably low as we can't fit a huge number of these images into VRAM on my laptop.\n",
    "\n",
    "Image size can be set here as I'm automatically resizing the images in my extraction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "\n",
    "image_size = width * height\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "code_sides = [12]\n",
    "\n",
    "convolution_filters = [8,6,4]\n",
    "\n",
    "model_path = \"../../Data/OPTIMAM_NEW/model0.pt\"\n",
    "\n",
    "#image_count = 500\n",
    "image_count = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "ImageFolder is used to load the base distribution images. This version of the DataLoader setup is designed to not batch or shuffle the images as we load them sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.total_imgs = natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"F\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor,Grayscale\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.0,65535.0)\n",
    "    ])\n",
    "\n",
    "root_dir = \"../../Data/OPTIMAM_NEW/png_images/casewise/ScreeningMammography/256/detector\"\n",
    "train_dataset = CustomDataSet(root_dir, transform)\n",
    "if (image_count == -1):\n",
    "    train_dataset_subset = train_dataset\n",
    "else:\n",
    "    train_dataset_subset = torch.utils.data.Subset(train_dataset, numpy.random.choice(len(train_dataset), image_count, replace=False))\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset_subset, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(platform)\n",
    "\n",
    "# reload the saved model\n",
    "\n",
    "model = torch.load(model_path,map_location=device)\n",
    "model.eval()\n",
    "\n",
    "code_size = code_sides[0] * code_sides[0]\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCELoss()\n",
    "\n",
    "losses = [None] * len(train_dataset_subset)\n",
    "encodings = [None] * len(train_dataset_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our autoencoder on the entire dataset and store the encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for batch_features, labels in train_loader:\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "\n",
    "        # compute reconstructions\n",
    "        code = model.encoder(batch_features)\n",
    "        outputs = model.decoder(code)\n",
    "        \n",
    "        code_reshaped = code.detach().cpu().numpy()[0]\n",
    "        code_reshaped.reshape(code_size)\n",
    "\n",
    "        encodings[count] = code_reshaped\n",
    "\n",
    "        # compute training reconstruction loss\n",
    "        error_criterion = criterion(outputs,batch_features)\n",
    "\n",
    "        losses[count] = error_criterion.cpu().numpy()\n",
    "\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate the encoding statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(encodings))\n",
    "print(len(encodings[0]))\n",
    "print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = np.mean(encodings,axis=0)\n",
    "feature_stds = np.std(encodings,axis=0)\n",
    "print(len(feature_means))\n",
    "print(len(feature_stds))\n",
    "\n",
    "mse_min = np.amin(losses)\n",
    "mse_max = np.amax(losses)\n",
    "mse_mean = np.mean(losses)\n",
    "print(\"MSE Min/Mean/Max:\" + str(mse_min) + \"/\" + str(mse_mean) + \"/\" + str(mse_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the compiled statistics to an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    np_losses = np.asarray(losses)\n",
    "\n",
    "    np_compiled = np.concatenate((np_losses[:, np.newaxis], encodings), axis=1)\n",
    "\n",
    "    np.savetxt('encodings.csv', encodings, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('losses.csv', np_losses, delimiter=',',fmt='%10.5f',newline='\\n')\n",
    "    np.savetxt('combined.csv', np_compiled, delimiter=',',fmt='%10.5f',newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Adversarials\n",
    "\n",
    "\n",
    "The next stage is to create a set of adversarial images. For this, we will use the same ImageFolder dataset as before. However, we will use a different Transform stack - with the same basic elements, plus a set of transforms intended to generate aberrational images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    trigger_chance = 0.8\n",
    "\n",
    "    PIL_transforms = torchvision.transforms.RandomApply([\n",
    "        torchvision.transforms.RandomAffine(degrees=15,translate=(0.4,0.4),shear=60),\n",
    "        torchvision.transforms.RandomVerticalFlip(),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.GaussianBlur(kernel_size=5),\n",
    "        torchvision.transforms.ColorJitter(brightness=0.8,contrast=0.8)\n",
    "        ],p=trigger_chance)\n",
    "\n",
    "    adversarial_transform = torchvision.transforms.Compose([\n",
    "         torchvision.transforms.Grayscale(),\n",
    "         torchvision.transforms.Resize((height,width)),\n",
    "         PIL_transforms,\n",
    "         torchvision.transforms.ToTensor(),\n",
    "         torchvision.transforms.RandomErasing(p=trigger_chance),\n",
    "        ])\n",
    "\n",
    "    adversarial_image_count = 5\n",
    "\n",
    "    adversarial_dataset = torchvision.datasets.ImageFolder(root=root_dir, transform=adversarial_transform)\n",
    "\n",
    "    adversarial_subset = torch.utils.data.Subset(adversarial_dataset, np.random.choice(len(adversarial_dataset), adversarial_image_count, replace=False))\n",
    "\n",
    "    adversarial_loader = torch.utils.data.DataLoader(\n",
    "        adversarial_subset, batch_size=1, shuffle=True\n",
    "    )\n",
    "    \n",
    "    adv_example_sets = np.zeros((5,height,width))\n",
    "    adv_code_sets = np.zeros((5,code_size))\n",
    "    reconstruction_sets = np.zeros((5,height,width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model on these adversarial images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for batch_features in adversarial_loader:\n",
    "        batch_features = batch_features[0]\n",
    "        adv_example = batch_features.to(device)\n",
    "        n_code = model.encoder(adv_example)\n",
    "        reconstruction = model(adv_example)\n",
    "    \n",
    "        adv_example_sets[i] = adv_example.cpu()\n",
    "        adv_code_sets[i] = n_code.cpu()\n",
    "        reconstruction_sets[i] = reconstruction.cpu()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, measure the loss and feature statistics for the adversarials:            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_array = ((reconstruction_sets - adv_example_sets)**2)\n",
    "mse = np.apply_over_axes(np.mean, mse_array, (1, 2))\n",
    "print(len(mse))\n",
    "print(mse)\n",
    "#sum_mse = np.sum(mse[0])\n",
    "#print(sum_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.ma.masked_equal(feature_stds,0)\n",
    "\n",
    "adv_divergence = (adv_code_sets - feature_means) / g\n",
    "print(np.max(adv_divergence,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    number = 5\n",
    "    plt.figure(figsize=(25, 9))\n",
    "    for index in range(number):\n",
    "        # display original\n",
    "        ax = plt.subplot(3, number, index + 1)\n",
    "        test_examples = adv_example_sets\n",
    "        copyback = test_examples[index]\n",
    "        plt.imshow(copyback.reshape(height, width))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display codes\n",
    "        ax = plt.subplot(3, number, index + 1 + number)\n",
    "        codes = adv_code_sets\n",
    "        code_copyback = codes[index]\n",
    "        plt.imshow(code_copyback.reshape(code_sides[0],code_sides[0]))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(3, number, index + 6 + number)\n",
    "        reconstruction = reconstruction_sets\n",
    "        recon_copyback = reconstruction[index]\n",
    "        plt.imshow(recon_copyback.reshape(height, width))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        out_path = \"adv_output\"+str(i)+\".png\" \n",
    "        plt.savefig(out_path)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
