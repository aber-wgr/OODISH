{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing an Autoencoder in PyTorch\n",
    "===\n",
    "\n",
    "This is adapted from the workbook provided alongside the article \"Implementing an Autoencoder in Pytorch\" which can be found [here](https://medium.com/pytorch/implementing-an-autoencoder-in-pytorch-19baa22647d1). The primary differences are that the network is much larger (as the code is designed to work with much larger images) and the model is split into two parts to allow for differential encode/decode metrics such as Mahalanobis Distance.\n",
    "\n",
    "This version of the model is designed with a convolutional model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "import math\n",
    "import numpy\n",
    "import collections\n",
    "\n",
    "from model import SplitAutoencoder,ExtensibleEncoder,ExtensibleDecoder\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our seed and other configurations for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    platform = \"cuda\"\n",
    "else:\n",
    "    platform = \"cpu\"\n",
    "print(platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the batch size, the number of training epochs, and the learning rate. Batch size has to be reasonably low as we can't fit a huge number of these images into VRAM on my laptop.\n",
    "\n",
    "Image size can be set here as I'm automatically resizing the images in my extraction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 256\n",
    "height = 256\n",
    "\n",
    "image_size = width * height\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 40\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#code_size = 100\n",
    "code_sides = [12,14,16]\n",
    "\n",
    "convolution_filters = [4,6,8]\n",
    "\n",
    "image_count = 300\n",
    "#image_count = -1\n",
    "\n",
    "validation_split = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Using a custom dataset class to load the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.total_imgs = natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"F\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torchvision.transforms import ToTensor,Grayscale\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.0,65535.0)\n",
    "    ])\n",
    "\n",
    "root_dir = \"../../Data/OPTIMAM_NEW/png_images/casewise/ScreeningMammography/256/detector\"\n",
    "#train_dataset = torchvision.datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "train_dataset = CustomDataSet(root_dir, transform)\n",
    "if (image_count == -1):\n",
    "    train_dataset_subset = train_dataset\n",
    "else:\n",
    "    train_dataset_subset = torch.utils.data.Subset(train_dataset, numpy.random.choice(len(train_dataset), image_count, replace=False))\n",
    "\n",
    "dataset_len = len(train_dataset_subset)\n",
    "indices = list(range(dataset_len))\n",
    "\n",
    "# Randomly splitting indices:\n",
    "val_len = int(np.floor((1.0 - validation_split) * dataset_len))\n",
    "\n",
    "dataset_size = len(train_dataset_subset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if 1 :\n",
    "    np.random.seed(1337)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, valid_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset_subset, batch_size=batch_size, sampler = train_sampler\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset_subset, batch_size=batch_size, sampler = valid_sampler\n",
    ")\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"val\": valid_loader}\n",
    "data_lengths = {\"train\": split, \"val\": val_len}\n",
    "print(split)\n",
    "print(val_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "run_device = torch.device(platform)\n",
    "store_device = torch.device(\"cpu\")\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "\n",
    "models = []\n",
    "optimizers = []\n",
    "\n",
    "for i in range(len(code_sides)):\n",
    "    models.append([])\n",
    "    optimizers.append([])\n",
    "    code_size = code_sides[i] * code_sides[i]\n",
    "    for j in range(len(convolution_filters)):\n",
    "        filters =  convolution_filters[j]\n",
    "        new_model = SplitAutoencoder(input_shape=(height,width),code_size=code_size,convolutions=filters).to(store_device)\n",
    "        models[i].append(new_model)\n",
    "        optimizers[i].append(optim.Adam(new_model.parameters(), lr=learning_rate))\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a grid parameter search method to train our autoencoder for our specified number of epochs for each combination of code sizes and convolutional filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1.0, None), (1.0, None), (1.0, None)], [(1.0, None), (1.0, None), (1.0, None)], [(1.0, None), (1.0, None), (1.0, None)]]\n"
     ]
    }
   ],
   "source": [
    "best_model_dicts = []\n",
    "# populate with fake best models\n",
    "for i in range(len(code_sides)):\n",
    "    best_model_dicts.append([])\n",
    "    for j in range(len(convolution_filters)):\n",
    "        best_model_dicts[i].append((1.0,None))\n",
    "print(best_model_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Running for code size:144 and filter size:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:92: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n",
      "E:\\miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/40, train loss = 0.05076135, validation loss = 0.04843598\n",
      "epoch : 2/40, train loss = 0.05077437, validation loss = 0.04831271\n",
      "epoch : 3/40, train loss = 0.05062513, validation loss = 0.04830639\n",
      "epoch : 4/40, train loss = 0.05041073, validation loss = 0.04846380\n",
      "epoch : 5/40, train loss = 0.05053440, validation loss = 0.04838316\n",
      "epoch : 6/40, train loss = 0.05066472, validation loss = 0.04826193\n",
      "epoch : 7/40, train loss = 0.05089676, validation loss = 0.04838356\n",
      "epoch : 8/40, train loss = 0.05053055, validation loss = 0.04839038\n",
      "epoch : 9/40, train loss = 0.04975608, validation loss = 0.04837431\n",
      "epoch : 10/40, train loss = 0.05089464, validation loss = 0.04828074\n",
      "epoch : 11/40, train loss = 0.05114251, validation loss = 0.04839160\n",
      "epoch : 12/40, train loss = 0.05103956, validation loss = 0.04830297\n",
      "epoch : 13/40, train loss = 0.05020082, validation loss = 0.04826523\n",
      "epoch : 14/40, train loss = 0.04981512, validation loss = 0.04851056\n",
      "epoch : 15/40, train loss = 0.05008516, validation loss = 0.04831929\n",
      "epoch : 16/40, train loss = 0.05004615, validation loss = 0.04833639\n",
      "epoch : 17/40, train loss = 0.05037367, validation loss = 0.04837977\n",
      "epoch : 18/40, train loss = 0.05123175, validation loss = 0.04830720\n",
      "epoch : 19/40, train loss = 0.05083090, validation loss = 0.04839521\n",
      "epoch : 20/40, train loss = 0.04935496, validation loss = 0.04838319\n",
      "epoch : 21/40, train loss = 0.05113017, validation loss = 0.04840207\n",
      "epoch : 22/40, train loss = 0.04989491, validation loss = 0.04828321\n",
      "epoch : 23/40, train loss = 0.05052441, validation loss = 0.04818451\n",
      "epoch : 24/40, train loss = 0.05091594, validation loss = 0.04831198\n",
      "epoch : 25/40, train loss = 0.05061929, validation loss = 0.04836001\n",
      "epoch : 26/40, train loss = 0.05029490, validation loss = 0.04830927\n",
      "epoch : 27/40, train loss = 0.05117208, validation loss = 0.04832482\n",
      "epoch : 28/40, train loss = 0.05133964, validation loss = 0.04839289\n",
      "epoch : 29/40, train loss = 0.05100839, validation loss = 0.04834346\n",
      "epoch : 30/40, train loss = 0.05054744, validation loss = 0.04839027\n",
      "epoch : 31/40, train loss = 0.05106993, validation loss = 0.04834268\n",
      "epoch : 32/40, train loss = 0.04965151, validation loss = 0.04840556\n",
      "epoch : 33/40, train loss = 0.05090131, validation loss = 0.04848953\n",
      "epoch : 34/40, train loss = 0.05073667, validation loss = 0.04830672\n",
      "epoch : 35/40, train loss = 0.05033686, validation loss = 0.04833590\n",
      "epoch : 36/40, train loss = 0.04998406, validation loss = 0.04842957\n",
      "epoch : 37/40, train loss = 0.05115918, validation loss = 0.04828398\n",
      "epoch : 38/40, train loss = 0.05040147, validation loss = 0.04826324\n",
      "epoch : 39/40, train loss = 0.05035824, validation loss = 0.04838887\n",
      "epoch : 40/40, train loss = 0.05140159, validation loss = 0.04839553\n",
      "==================\n",
      "Running for code size:144 and filter size:6\n",
      "epoch : 1/40, train loss = 0.05076267, validation loss = 0.04834427\n",
      "epoch : 2/40, train loss = 0.05026824, validation loss = 0.04838475\n",
      "epoch : 3/40, train loss = 0.05106583, validation loss = 0.04825733\n",
      "epoch : 4/40, train loss = 0.05054175, validation loss = 0.04843342\n",
      "epoch : 5/40, train loss = 0.05006821, validation loss = 0.04830724\n",
      "epoch : 6/40, train loss = 0.05060689, validation loss = 0.04829733\n",
      "epoch : 7/40, train loss = 0.04975703, validation loss = 0.04834565\n",
      "epoch : 8/40, train loss = 0.05028056, validation loss = 0.04833927\n",
      "epoch : 9/40, train loss = 0.05050760, validation loss = 0.04834322\n",
      "epoch : 10/40, train loss = 0.05054496, validation loss = 0.04840695\n",
      "epoch : 11/40, train loss = 0.05071628, validation loss = 0.04835839\n",
      "epoch : 12/40, train loss = 0.05068888, validation loss = 0.04833340\n",
      "epoch : 13/40, train loss = 0.05041221, validation loss = 0.04829934\n",
      "epoch : 14/40, train loss = 0.05062652, validation loss = 0.04832447\n",
      "epoch : 15/40, train loss = 0.05062875, validation loss = 0.04832377\n",
      "epoch : 16/40, train loss = 0.05077582, validation loss = 0.04838024\n",
      "epoch : 17/40, train loss = 0.05201509, validation loss = 0.04832439\n",
      "epoch : 18/40, train loss = 0.05052840, validation loss = 0.04836757\n",
      "epoch : 19/40, train loss = 0.05069512, validation loss = 0.04827961\n",
      "epoch : 20/40, train loss = 0.05064646, validation loss = 0.04822908\n",
      "epoch : 21/40, train loss = 0.05131320, validation loss = 0.04830060\n",
      "epoch : 22/40, train loss = 0.05047004, validation loss = 0.04827653\n",
      "epoch : 23/40, train loss = 0.04999789, validation loss = 0.04837139\n",
      "epoch : 24/40, train loss = 0.05113928, validation loss = 0.04830356\n",
      "epoch : 25/40, train loss = 0.05017725, validation loss = 0.04841663\n",
      "epoch : 26/40, train loss = 0.05091475, validation loss = 0.04831246\n",
      "epoch : 27/40, train loss = 0.05006426, validation loss = 0.04838473\n",
      "epoch : 28/40, train loss = 0.05079255, validation loss = 0.04825012\n",
      "epoch : 29/40, train loss = 0.05063330, validation loss = 0.04831896\n",
      "epoch : 30/40, train loss = 0.05078790, validation loss = 0.04839255\n",
      "epoch : 31/40, train loss = 0.04973162, validation loss = 0.04836020\n",
      "epoch : 32/40, train loss = 0.05021934, validation loss = 0.04829366\n",
      "epoch : 33/40, train loss = 0.05076579, validation loss = 0.04827430\n",
      "epoch : 34/40, train loss = 0.05114151, validation loss = 0.04852439\n",
      "epoch : 35/40, train loss = 0.05132457, validation loss = 0.04830016\n",
      "epoch : 36/40, train loss = 0.05113595, validation loss = 0.04839495\n",
      "epoch : 37/40, train loss = 0.05080231, validation loss = 0.04836652\n",
      "epoch : 38/40, train loss = 0.04921052, validation loss = 0.04839786\n",
      "epoch : 39/40, train loss = 0.05125270, validation loss = 0.04832545\n",
      "epoch : 40/40, train loss = 0.05050262, validation loss = 0.04829368\n",
      "==================\n",
      "Running for code size:144 and filter size:8\n",
      "epoch : 1/40, train loss = 0.05073080, validation loss = 0.04828153\n",
      "epoch : 2/40, train loss = 0.05085455, validation loss = 0.04845597\n",
      "epoch : 3/40, train loss = 0.05035033, validation loss = 0.04834823\n",
      "epoch : 4/40, train loss = 0.05091450, validation loss = 0.04835485\n",
      "epoch : 5/40, train loss = 0.05031644, validation loss = 0.04843127\n",
      "epoch : 6/40, train loss = 0.05087596, validation loss = 0.04834924\n",
      "epoch : 7/40, train loss = 0.05038740, validation loss = 0.04833098\n",
      "epoch : 8/40, train loss = 0.05063125, validation loss = 0.04828314\n",
      "epoch : 9/40, train loss = 0.05132104, validation loss = 0.04832586\n",
      "epoch : 10/40, train loss = 0.05009237, validation loss = 0.04830818\n",
      "epoch : 11/40, train loss = 0.05032481, validation loss = 0.04834684\n",
      "epoch : 12/40, train loss = 0.05020958, validation loss = 0.04830583\n",
      "epoch : 13/40, train loss = 0.05076958, validation loss = 0.04840575\n",
      "epoch : 14/40, train loss = 0.05137932, validation loss = 0.04841614\n",
      "epoch : 15/40, train loss = 0.05179866, validation loss = 0.04829771\n",
      "epoch : 16/40, train loss = 0.05138934, validation loss = 0.04836350\n",
      "epoch : 17/40, train loss = 0.05095979, validation loss = 0.04840465\n",
      "epoch : 18/40, train loss = 0.05032918, validation loss = 0.04840759\n",
      "epoch : 19/40, train loss = 0.05044835, validation loss = 0.04840876\n",
      "epoch : 20/40, train loss = 0.05018657, validation loss = 0.04831464\n",
      "epoch : 21/40, train loss = 0.05080827, validation loss = 0.04843045\n",
      "epoch : 22/40, train loss = 0.05154488, validation loss = 0.04829632\n",
      "epoch : 23/40, train loss = 0.05080706, validation loss = 0.04837522\n",
      "epoch : 24/40, train loss = 0.05072317, validation loss = 0.04838758\n",
      "epoch : 25/40, train loss = 0.05052233, validation loss = 0.04830955\n",
      "epoch : 26/40, train loss = 0.05069096, validation loss = 0.04837596\n",
      "epoch : 27/40, train loss = 0.05079475, validation loss = 0.04831271\n",
      "epoch : 28/40, train loss = 0.05024257, validation loss = 0.04833965\n",
      "epoch : 29/40, train loss = 0.05067220, validation loss = 0.04847109\n",
      "epoch : 30/40, train loss = 0.05108024, validation loss = 0.04827699\n",
      "epoch : 31/40, train loss = 0.05033769, validation loss = 0.04842363\n",
      "epoch : 32/40, train loss = 0.05006230, validation loss = 0.04829513\n",
      "epoch : 33/40, train loss = 0.05048864, validation loss = 0.04831771\n",
      "epoch : 34/40, train loss = 0.05128676, validation loss = 0.04832459\n",
      "epoch : 35/40, train loss = 0.05063387, validation loss = 0.04829783\n",
      "epoch : 36/40, train loss = 0.05087781, validation loss = 0.04835994\n",
      "epoch : 37/40, train loss = 0.05082224, validation loss = 0.04839623\n",
      "epoch : 38/40, train loss = 0.05117843, validation loss = 0.04841416\n",
      "epoch : 39/40, train loss = 0.05003267, validation loss = 0.04845010\n",
      "epoch : 40/40, train loss = 0.05069599, validation loss = 0.04832179\n",
      "==================\n",
      "Running for code size:196 and filter size:4\n",
      "epoch : 1/40, train loss = 0.04741967, validation loss = 0.04322550\n",
      "epoch : 2/40, train loss = 0.04173864, validation loss = 0.03589425\n",
      "epoch : 3/40, train loss = 0.03541891, validation loss = 0.03436884\n",
      "epoch : 4/40, train loss = 0.03281592, validation loss = 0.03338253\n",
      "epoch : 5/40, train loss = 0.03191246, validation loss = 0.03304399\n",
      "epoch : 6/40, train loss = 0.03069680, validation loss = 0.03241700\n",
      "epoch : 7/40, train loss = 0.03094868, validation loss = 0.03233488\n",
      "epoch : 8/40, train loss = 0.03086865, validation loss = 0.03222890\n",
      "epoch : 9/40, train loss = 0.03028890, validation loss = 0.03212741\n",
      "epoch : 10/40, train loss = 0.03029294, validation loss = 0.03116044\n",
      "epoch : 11/40, train loss = 0.03017090, validation loss = 0.03119253\n",
      "epoch : 12/40, train loss = 0.02939484, validation loss = 0.03132790\n",
      "epoch : 13/40, train loss = 0.02939028, validation loss = 0.03162966\n",
      "epoch : 14/40, train loss = 0.02907115, validation loss = 0.03049034\n",
      "epoch : 15/40, train loss = 0.02777198, validation loss = 0.02981593\n",
      "epoch : 16/40, train loss = 0.02697117, validation loss = 0.02902214\n",
      "epoch : 17/40, train loss = 0.02600681, validation loss = 0.02745541\n",
      "epoch : 18/40, train loss = 0.02378867, validation loss = 0.02552031\n",
      "epoch : 19/40, train loss = 0.02151128, validation loss = 0.02282460\n",
      "epoch : 20/40, train loss = 0.01860774, validation loss = 0.01943019\n",
      "epoch : 21/40, train loss = 0.01587324, validation loss = 0.01720586\n",
      "epoch : 22/40, train loss = 0.01412768, validation loss = 0.01588126\n",
      "epoch : 23/40, train loss = 0.01322655, validation loss = 0.01540281\n",
      "epoch : 24/40, train loss = 0.01277512, validation loss = 0.01495369\n",
      "epoch : 25/40, train loss = 0.01243394, validation loss = 0.01455911\n",
      "epoch : 26/40, train loss = 0.01213859, validation loss = 0.01424146\n",
      "epoch : 27/40, train loss = 0.01173843, validation loss = 0.01407931\n",
      "epoch : 28/40, train loss = 0.01147349, validation loss = 0.01381393\n",
      "epoch : 29/40, train loss = 0.01133298, validation loss = 0.01356230\n",
      "epoch : 30/40, train loss = 0.01092811, validation loss = 0.01337003\n",
      "epoch : 31/40, train loss = 0.01062984, validation loss = 0.01330499\n",
      "epoch : 32/40, train loss = 0.01054289, validation loss = 0.01310632\n",
      "epoch : 33/40, train loss = 0.01021088, validation loss = 0.01288122\n",
      "epoch : 34/40, train loss = 0.00993022, validation loss = 0.01277821\n",
      "epoch : 35/40, train loss = 0.00983628, validation loss = 0.01266786\n",
      "epoch : 36/40, train loss = 0.00963919, validation loss = 0.01241795\n",
      "epoch : 37/40, train loss = 0.00942845, validation loss = 0.01236705\n",
      "epoch : 38/40, train loss = 0.00923435, validation loss = 0.01220339\n",
      "epoch : 39/40, train loss = 0.00897167, validation loss = 0.01209394\n",
      "epoch : 40/40, train loss = 0.00863156, validation loss = 0.01201031\n",
      "==================\n",
      "Running for code size:196 and filter size:6\n",
      "epoch : 1/40, train loss = 0.03620393, validation loss = 0.03364345\n",
      "epoch : 2/40, train loss = 0.03261178, validation loss = 0.03123001\n",
      "epoch : 3/40, train loss = 0.02867819, validation loss = 0.02723045\n",
      "epoch : 4/40, train loss = 0.02282859, validation loss = 0.02162757\n",
      "epoch : 5/40, train loss = 0.01829849, validation loss = 0.01856248\n",
      "epoch : 6/40, train loss = 0.01594502, validation loss = 0.01665117\n",
      "epoch : 7/40, train loss = 0.01418452, validation loss = 0.01563364\n",
      "epoch : 8/40, train loss = 0.01328717, validation loss = 0.01486195\n",
      "epoch : 9/40, train loss = 0.01235664, validation loss = 0.01422314\n",
      "epoch : 10/40, train loss = 0.01185234, validation loss = 0.01390470\n",
      "epoch : 11/40, train loss = 0.01121953, validation loss = 0.01354221\n",
      "epoch : 12/40, train loss = 0.01098195, validation loss = 0.01335939\n",
      "epoch : 13/40, train loss = 0.01018047, validation loss = 0.01284743\n",
      "epoch : 14/40, train loss = 0.00973786, validation loss = 0.01252364\n",
      "epoch : 15/40, train loss = 0.00933082, validation loss = 0.01233488\n",
      "epoch : 16/40, train loss = 0.00876603, validation loss = 0.01210082\n",
      "epoch : 17/40, train loss = 0.00853948, validation loss = 0.01185004\n",
      "epoch : 18/40, train loss = 0.00805965, validation loss = 0.01171412\n",
      "epoch : 19/40, train loss = 0.00772706, validation loss = 0.01149994\n",
      "epoch : 20/40, train loss = 0.00750310, validation loss = 0.01144647\n",
      "epoch : 21/40, train loss = 0.00710720, validation loss = 0.01123716\n",
      "epoch : 22/40, train loss = 0.00676557, validation loss = 0.01102608\n",
      "epoch : 23/40, train loss = 0.00654746, validation loss = 0.01099249\n",
      "epoch : 24/40, train loss = 0.00619012, validation loss = 0.01089769\n",
      "epoch : 25/40, train loss = 0.00592540, validation loss = 0.01081706\n",
      "epoch : 26/40, train loss = 0.00567693, validation loss = 0.01081623\n",
      "epoch : 27/40, train loss = 0.00535193, validation loss = 0.01075272\n",
      "epoch : 28/40, train loss = 0.00521781, validation loss = 0.01073907\n",
      "epoch : 29/40, train loss = 0.00501611, validation loss = 0.01070685\n",
      "epoch : 30/40, train loss = 0.00484310, validation loss = 0.01068628\n",
      "epoch : 31/40, train loss = 0.00462409, validation loss = 0.01070430\n",
      "epoch : 32/40, train loss = 0.00453938, validation loss = 0.01069933\n",
      "epoch : 33/40, train loss = 0.00433494, validation loss = 0.01076552\n",
      "epoch : 34/40, train loss = 0.00417071, validation loss = 0.01072708\n",
      "epoch : 35/40, train loss = 0.00409769, validation loss = 0.01072052\n",
      "epoch : 36/40, train loss = 0.00404147, validation loss = 0.01072948\n",
      "epoch : 37/40, train loss = 0.00396656, validation loss = 0.01087030\n",
      "epoch : 38/40, train loss = 0.00375499, validation loss = 0.01073408\n",
      "epoch : 39/40, train loss = 0.00362966, validation loss = 0.01080309\n",
      "epoch : 40/40, train loss = 0.00351347, validation loss = 0.01082458\n",
      "==================\n",
      "Running for code size:196 and filter size:8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 8.00 GiB total capacity; 6.29 GiB already allocated; 120.75 MiB free; 6.45 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ed7a8f5193b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                         \u001b[1;31m# compute accumulated gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                         \u001b[0mlocal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                         \u001b[1;31m# perform parameter update based on current gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\miniconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 8.00 GiB total capacity; 6.29 GiB already allocated; 120.75 MiB free; 6.45 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []        \n",
    "for i in range(len(code_sides)):\n",
    "    train_losses.append([])\n",
    "    val_losses.append([])\n",
    "\n",
    "    for j in range(len(convolution_filters)):\n",
    "        print(\"==================\")\n",
    "        \n",
    "        print(\"Running for code size:\" + str(code_sides[i] * code_sides[i]) + \" and filter size:\"+str(convolution_filters[j]))\n",
    "        train_losses[i].append([])\n",
    "        val_losses[i].append([])\n",
    "        \n",
    "        model = models[i][j].to(run_device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            losses = {'train':0.0, 'val':0.0}\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "                for batch_features in data_loaders[phase]:\n",
    "                    # load it to the active device\n",
    "                    batch_features = batch_features.to(run_device)\n",
    "\n",
    "                    # reset the gradients back to zero\n",
    "                    # PyTorch accumulates gradients on subsequent backward passes\n",
    "                    optimizers[i][j].zero_grad()\n",
    "\n",
    "                    # compute reconstructions\n",
    "                    #print(batch_features.size())\n",
    "                    codes = model.encoder(batch_features)\n",
    "                    outputs = model.decoder(codes)\n",
    "\n",
    "                    # compute training reconstruction loss\n",
    "                    local_loss = criterion(outputs,batch_features)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        # compute accumulated gradients\n",
    "                        local_loss.backward()\n",
    "\n",
    "                        # perform parameter update based on current gradients\n",
    "                        optimizers[i][j].step()\n",
    "\n",
    "                    # add the mini-batch training loss to epoch loss\n",
    "                    losses[phase] += local_loss.item()\n",
    "\n",
    "            # compute the epoch training loss\n",
    "            #losses['train'] = losses['train'] / data_lengths['train']\n",
    "            #losses['val'] = losses['val'] / data_lengths['val']\n",
    "\n",
    "            losses['train'] = losses['train'] / len(data_loaders['train'])\n",
    "            losses['val'] = losses['val'] / len(data_loaders['val'])\n",
    "\n",
    "            #check if best model\n",
    "            if(losses['val'] < best_model_dicts[i][j][0]):\n",
    "                best_model_dicts[i][j] = (losses['val'],models[i][j].state_dict())\n",
    "\n",
    "            train_losses[i][j].append(losses['train'])\n",
    "            val_losses[i][j].append(losses['val'])\n",
    "\n",
    "            # display the epoch training loss\n",
    "            print(\"epoch : {}/{}, train loss = {:.8f}, validation loss = {:.8f}\".format(epoch + 1, epochs, losses['train'],losses['val']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the best trained model and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(code_sides)):\n",
    "    if(best_model_dicts[i][j][1]!=None):\n",
    "        models[i][j].load_state_dict(best_model_dicts[i][j][1])\n",
    "        PATH = \"../../Data/OPTIMAM_NEW/model\" + str(i) + \"_\" + str(j) +\".pt\"\n",
    "        torch.save(models[i][j], PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract some test examples to reconstruct using our trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataSet(root_dir, transform) # same transform as we used for the training, for compatibility\n",
    "#test_dataset = train_dataset_subset\n",
    "if (image_count == -1):\n",
    "    test_dataset_subset = test_dataset\n",
    "else:\n",
    "    test_dataset_subset = torch.utils.data.Subset(test_dataset, numpy.random.choice(len(test_dataset), image_count, replace=False))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset_subset, batch_size=5, shuffle=True\n",
    ")\n",
    "\n",
    "test_example_sets = [None] * len(code_sides)\n",
    "code_sets = [None] * len(code_sides)\n",
    "reconstruction_sets = [None] * len(code_sides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's try to reconstruct some test images using our trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(len(code_sides)):\n",
    "        for batch_features in test_loader:\n",
    "            #batch_features = batch_features[0]\n",
    "            test_examples = batch_features.to(device)\n",
    "            n_codes = models[i].encoder(test_examples)\n",
    "            reconstruction = models[i](test_examples)\n",
    "            break;\n",
    "        test_example_sets[i] = test_examples\n",
    "        code_sets[i] = n_codes\n",
    "        reconstruction_sets[i] = reconstruction\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(len(code_sides)):\n",
    "        number = 5\n",
    "        plt.figure(figsize=(25, 9))\n",
    "        for index in range(number):\n",
    "            # display original\n",
    "            ax = plt.subplot(3, number, index + 1)\n",
    "            test_examples = test_example_sets[i]\n",
    "            copyback = test_examples[index].cpu()\n",
    "            #plt.imshow(copyback.numpy().reshape(height, width), vmin=0, vmax=65535)\n",
    "            plt.imshow(copyback.reshape(height, width))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "            # display codes\n",
    "            ax = plt.subplot(3, number, index + 1 + number)\n",
    "            codes = code_sets[i]\n",
    "            code_copyback = codes[index].cpu()\n",
    "            plt.imshow(code_copyback.numpy().reshape(code_sides[i],code_sides[i]))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "            # display reconstruction\n",
    "            ax = plt.subplot(3, number, index + 6 + number)\n",
    "            reconstruction = reconstruction_sets[i]\n",
    "            recon_copyback = reconstruction[index].cpu()\n",
    "            plt.imshow(recon_copyback.reshape(height, width))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        out_path = \"output\"+str(i)+\".png\" \n",
    "        plt.savefig(out_path)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
